{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workspace/tools/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "import lightgbm\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mname = 'lgb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape 161570557,13\n",
      "df_val.shape 23333333,13\n",
      "df_sub.shape 18790469,13\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_sub = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.externals import joblib\\nohe = joblib.load('../model/ohe.pickle') \\nencoder = keras.models.load_model('../model/encoder')\\n\\ndef autoencoder(tdf):\\n    return encoder.predict(ohe.transform(tdf[xnames[:5]]), batch_size=256, verbose=1)\\n\\nsample = 4000000\\n\\ndata_val = lightgbm.Dataset(autoencoder(df_val[: sample]), df_val[yname][:sample])\\ndata = lightgbm.Dataset(autoencoder(df_train[: sample]), df_train[yname][:sample])\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.externals import joblib\n",
    "ohe = joblib.load('../model/ohe.pickle') \n",
    "encoder = keras.models.load_model('../model/encoder')\n",
    "\n",
    "def autoencoder(tdf):\n",
    "    return encoder.predict(ohe.transform(tdf[xnames[:5]]), batch_size=256, verbose=1)\n",
    "\n",
    "sample = 4000000\n",
    "\n",
    "data_val = lightgbm.Dataset(autoencoder(df_val[: sample]), df_val[yname][:sample])\n",
    "data = lightgbm.Dataset(autoencoder(df_train[: sample]), df_train[yname][:sample])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.1,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': 5,  # -1 means no limit\n",
    "        'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'scale_pos_weight': 99, # because training data is extremely unbalanced \n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = lightgbm.Dataset(df_val[xnames], df_val[yname], categorical_feature=categorical_names)\n",
    "data = lightgbm.Dataset(df_train[xnames], df_train[yname], categorical_feature=categorical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workspace/tools/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/ubuntu/workspace/tools/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain's auc: 0.953519\tvalid's auc: 0.955923\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[2]\ttrain's auc: 0.958712\tvalid's auc: 0.96026\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2]\ttrain's auc: 0.958712\tvalid's auc: 0.96026\n"
     ]
    }
   ],
   "source": [
    "evals_result={}\n",
    "r = lightgbm.train(params,\n",
    "                   data,\n",
    "                   valid_sets=[data, data_val],\n",
    "                   valid_names=['train', 'valid'], \n",
    "                   evals_result=evals_result,\n",
    "                   num_boost_round=300,\n",
    "                   early_stopping_rounds=30\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(r, '../model/%s.pkl' % mname) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = r.predict(df_val[xnames].values, num_iteration=r.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing val ...\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print('writing val ...')\n",
    "output_val = pd.DataFrame({'y_pred': y_val_pred, 'y': df_val[yname].astype(int).values})\n",
    "with open('../data/%s_val.pickle' % mname, 'wb') as handle:\n",
    "    pickle.dump(output_val, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting...\n",
      "writing...\n"
     ]
    }
   ],
   "source": [
    "ids = df_sub['click_id'].astype(int).values\n",
    "X_sub = df_sub[xnames].values\n",
    "print('predicting...')\n",
    "y_sub = r.predict(X_sub, num_iteration=r.best_iteration)\n",
    "print('writing...')\n",
    "output = pd.DataFrame({'click_id': ids, 'is_attributed': y_sub})\n",
    "output.to_csv(\"../data/%s.csv.gz\" % mname, compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
